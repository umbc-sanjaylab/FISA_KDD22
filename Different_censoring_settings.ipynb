{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ece758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "        \n",
    "# Set the random seed\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c2fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_cenored_data_support(support_data_csv_file):\n",
    "    data=pd.read_csv(support_data_csv_file) #Load the data\n",
    "    data_uncen=data.loc[data[\"status\"]==1,:] #Create uncensored data by extracting uncensored subjects from the original data\n",
    "    data_cen=data.loc[data[\"status\"]==0,:]  #Create censored data by extracting censored subjects from the original data\n",
    "    \n",
    "    ## Print the summary of the dataset\n",
    "    print('Dimension of uncensored:',data_uncen.shape)\n",
    "    print('Dimension of censored:',data_cen.shape)\n",
    "    print('Percentage of white censored: {:.2f}%'.format((data_cen.loc[data_cen[\"race\"]==1,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('Percentage of non-white censored: {:.2f}%'.format((data_cen.loc[data_cen[\"race\"]==2,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('No. of uncensored white:',data_uncen.loc[data_uncen[\"race\"]==1,:].shape[0])\n",
    "    print('No. of uncensored non-white:',data_uncen.loc[data_uncen[\"race\"]==2,:].shape[0])\n",
    "    print('No. of censored white:',data_cen.loc[data_cen[\"race\"]==1,:].shape[0])\n",
    "    print('No. of censored non-white:',data_cen.loc[data_cen[\"race\"]==2,:].shape[0])   \n",
    "    \n",
    "    \n",
    "    uncen_white_1000=data_uncen.loc[data_uncen[\"race\"]==1,:].sample(n = 1000) #Randomly select 1000 uncenored subjects from the white group of uncensored data\n",
    "    uncen_non_white_1000=data_uncen.loc[data_uncen[\"race\"]==2,:].sample(n = 1000) #Randomly select 1000 uncenored subjects from the non-white group of uncensored data\n",
    "    uncenored_data=pd.concat([uncen_white_1000, uncen_non_white_1000])  #Concatenate randomly selected 1000+1000=2000 uncensored subjects from the white and non-white group of uncensored data\n",
    "    increment_cenored_white_500=data_cen.loc[data_cen[\"race\"]==1,:].sample(n = 500) #Select 500 cenored subjects from the white group of censored data\n",
    "    increment_cenored_non_white_500=data_cen.loc[data_cen[\"race\"]==2,:].sample(n = 500) #Select 500 cenored subjects from the non-white group of censored data\n",
    "    increment_all=pd.concat([uncenored_data, increment_cenored_white_500, increment_cenored_non_white_500]).sample(frac=1.0) ## Create Increment All data by respectively adding 500 censored cenored subjects from the white group and non-white group of censored data to uncensored dataset\n",
    "    increment_majority=pd.concat([uncenored_data, increment_cenored_white_500]).sample(frac=1.0) ## Create Increment Majority data by only adding 500 censored cenored subjects from the white group of censored data to uncensored dataset\n",
    "    increment_minority=pd.concat([uncenored_data, increment_cenored_non_white_500]).sample(frac=1.0) ## Create Increment Minority data by only adding 500 censored cenored subjects from the non-white group of censored data to uncensored dataset\n",
    "\n",
    "\n",
    "    induced_cenored_white_500=uncen_white_1000.sample(n = 500) #Randomly select 500 cenored subjects from the white group of uncensored data\n",
    "    induced_cenored_white_500['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_white_500=uncen_white_1000[~uncen_white_1000.index.isin(induced_cenored_white_500.index)] #Extract the white uncensored data where censoring is not induced\n",
    "\n",
    "    induced_cenored_non_white_500=uncen_non_white_1000.sample(n = 500) #Randomly select 500 cenored subjects from the non-white group of uncensored data\n",
    "    induced_cenored_non_white_500['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_non_white_500=uncen_non_white_1000[~uncen_non_white_1000.index.isin(induced_cenored_non_white_500.index)] #Extract the non-white uncensored data where censoring is not induced\n",
    "\n",
    "\n",
    "    induced_majority=pd.concat([induced_uncenored_white_500, induced_cenored_white_500, uncen_non_white_1000]).sample(frac=1.0) ## Create Induced Majority data by inducing 500 censored subjects to the white group of uncensored data \n",
    "    induced_minority=pd.concat([induced_uncenored_non_white_500, induced_cenored_non_white_500, uncen_white_1000]).sample(frac=1.0) ## Create Induced Minority data by inducing 500 censored subjects to the non-white group of uncensored data \n",
    "    \n",
    "    return uncenored_data, increment_majority, increment_minority, induced_majority, induced_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3bc93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of uncensored: (6168, 34)\n",
      "Dimension of censored: (2894, 34)\n",
      "Percentage of white censored: 77.51%\n",
      "Percentage of non-white censored: 22.49%\n",
      "No. of uncensored white: 4947\n",
      "No. of uncensored non-white: 1221\n",
      "No. of censored white: 2243\n",
      "No. of censored non-white: 651\n"
     ]
    }
   ],
   "source": [
    "uncenored_data, increment_majority, increment_minority, induced_majority, induced_minority=different_cenored_data_support('/home/local/AD/mrahman6/0.KDD_2022/0.FISA_FINAL_EXPERIMENT/Data/SUPPORT/support.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4941c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_cenored_flchain(flchain_data_csv_file):\n",
    "    data=pd.read_csv(flchain_data_csv_file) #Load the data\n",
    "    data_uncen=data.loc[data[\"status\"]==1,:] #Create uncensored data by extracting uncensored subjects from the original data\n",
    "    data_cen=data.loc[data[\"status\"]==0,:]  #Create censored data by extracting censored subjects from the original data\n",
    "    \n",
    "    ## Print the summary of the dataset\n",
    "    print('Dimension of uncensored:',data_uncen.shape)\n",
    "    print('Dimension of censored:',data_cen.shape)\n",
    "    print('Percentage of male censored: {:.2f}%'.format((data_cen.loc[data_cen[\"sex\"]==1,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('Percentage of female censored: {:.2f}%'.format((data_cen.loc[data_cen[\"sex\"]==0,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('No. of uncensored male:',data_uncen.loc[data_uncen[\"sex\"]==1,:].shape[0])\n",
    "    print('No. of uncensored female:',data_uncen.loc[data_uncen[\"sex\"]==0,:].shape[0])\n",
    "    print('No. of censored male:',data_cen.loc[data_cen[\"sex\"]==1,:].shape[0])\n",
    "    print('No. of censored female:',data_cen.loc[data_cen[\"sex\"]==0,:].shape[0])   \n",
    "    \n",
    "    \n",
    "    uncen_male_500=data_uncen.loc[data_uncen[\"sex\"]==1,:].sample(n = 500) #Randomly select 500 uncenored subjects from the male group of uncensored data\n",
    "    uncen_female_500=data_uncen.loc[data_uncen[\"sex\"]==0,:].sample(n = 500) #Randomly select 500 uncenored subjects from the female group of uncensored data\n",
    "    uncenored_data=pd.concat([uncen_male_500, uncen_female_500])  #Concatenate randomly selected 500+500=1000 uncensored subjects from the male and female group of uncensored data\n",
    "    increment_cenored_male_250=data_cen.loc[data_cen[\"sex\"]==1,:].sample(n = 250) #Select 250 cenored subjects from the male group of censored data\n",
    "    increment_cenored_female_250=data_cen.loc[data_cen[\"sex\"]==0,:].sample(n = 250) #Select 250 cenored subjects from the female group of censored data\n",
    "    increment_all=pd.concat([uncenored_data, increment_cenored_male_250, increment_cenored_female_250]).sample(frac=1.0) ## Create Increment All data by respectively adding 500 censored cenored subjects from the male group and female group of censored data to uncensored dataset\n",
    "    increment_majority=pd.concat([uncenored_data, increment_cenored_male_250]).sample(frac=1.0) ## Create Increment Majority data by only adding 250 censored cenored subjects from the male group of censored data to uncensored dataset\n",
    "    increment_minority=pd.concat([uncenored_data, increment_cenored_female_250]).sample(frac=1.0) ## Create Increment Minority data by only adding 250 censored cenored subjects from the female group of censored data to uncensored dataset\n",
    "\n",
    "\n",
    "    induced_cenored_male_250=uncen_male_500.sample(n = 250) #Randomly select 500 cenored subjects from the male group of uncensored data\n",
    "    induced_cenored_male_250['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_male_250=uncen_male_500[~uncen_male_500.index.isin(induced_cenored_male_250.index)] #Extract the male uncensored data where censoring is not induced\n",
    "\n",
    "    induced_cenored_female_250=uncen_female_500.sample(n = 250) #Randomly select 500 cenored subjects from the female group of uncensored data\n",
    "    induced_cenored_female_250['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_female_250=uncen_female_500[~uncen_female_500.index.isin(induced_cenored_female_250.index)] #Extract the female uncensored data where censoring is not induced\n",
    "\n",
    "\n",
    "    induced_majority=pd.concat([induced_uncenored_male_250, induced_cenored_male_250, uncen_female_500]).sample(frac=1.0) ## Create Induced Majority data by inducing 500 censored subjects to the male group of uncensored data \n",
    "    induced_minority=pd.concat([induced_uncenored_female_250, induced_cenored_female_250, uncen_male_500]).sample(frac=1.0) ## Create Induced Minority data by inducing 500 censored subjects to the female group of uncensored data \n",
    "    \n",
    "    return uncenored_data, increment_majority, increment_minority, induced_majority, induced_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105e5b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of uncensored: (1959, 10)\n",
      "Dimension of censored: (4562, 10)\n",
      "Percentage of male censored: 44.76%\n",
      "Percentage of female censored: 55.24%\n",
      "No. of uncensored male: 890\n",
      "No. of uncensored female: 1069\n",
      "No. of censored male: 2042\n",
      "No. of censored female: 2520\n"
     ]
    }
   ],
   "source": [
    "uncenored_data, increment_majority, increment_minority, induced_majority, induced_minority=different_cenored_flchain('/home/local/AD/mrahman6/0.KDD_2022/0.FISA_FINAL_EXPERIMENT/Data/FLC/flchain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08cf3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_cenored_seer(seer_data_csv_file):\n",
    "    data=pd.read_csv(seer_data_csv_file) #Load the data\n",
    "    data=data[data['Race_ord']!=5]\n",
    "    data_uncen=data.loc[data[\"status\"]==1,:] #Create uncensored data by extracting uncensored subjects from the original data\n",
    "    data_cen=data.loc[data[\"status\"]==0,:]  #Create censored data by extracting censored subjects from the original data\n",
    "    \n",
    "    ## Print the summary of the dataset\n",
    "    print('Dimension of uncensored:',data_uncen.shape)\n",
    "    print('Dimension of censored:',data_cen.shape)\n",
    "    print('Percentage of white censored: {:.2f}%'.format((data_cen.loc[data_cen[\"Race_ord\"]==1,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('Percentage of black censored: {:.2f}%'.format((data_cen.loc[data_cen[\"Race_ord\"]==2,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('Percentage of asian censored: {:.2f}%'.format((data_cen.loc[data_cen[\"Race_ord\"]==3,:].shape[0])/(data_cen.shape[0])*100))\n",
    "    print('Percentage of hispanic censored: {:.2f}%'.format((data_cen.loc[data_cen[\"Race_ord\"]==4,:].shape[0])/(data_cen.shape[0])*100))\n",
    "\n",
    "    \n",
    "    print('No. of uncensored white:',data_uncen.loc[data_uncen[\"Race_ord\"]==1,:].shape[0])\n",
    "    print('No. of uncensored black:',data_uncen.loc[data_uncen[\"Race_ord\"]==2,:].shape[0])\n",
    "    print('No. of uncensored asian:',data_uncen.loc[data_uncen[\"Race_ord\"]==3,:].shape[0])\n",
    "    print('No. of uncensored hispanic:',data_uncen.loc[data_uncen[\"Race_ord\"]==4,:].shape[0])    \n",
    "    \n",
    "    \n",
    "    print('No. of censored white:',data_cen.loc[data_cen[\"Race_ord\"]==1,:].shape[0])\n",
    "    print('No. of censored black:',data_cen.loc[data_cen[\"Race_ord\"]==2,:].shape[0])   \n",
    "    print('No. of censored asian:',data_cen.loc[data_cen[\"Race_ord\"]==3,:].shape[0])\n",
    "    print('No. of censored hispanic:',data_cen.loc[data_cen[\"Race_ord\"]==4,:].shape[0])       \n",
    "    \n",
    "    uncen_white_500=data_uncen.loc[data_uncen[\"Race_ord\"]==1,:].sample(n = 500) #Randomly select 500 uncenored subjects from the white group of uncensored data\n",
    "    uncen_black_500=data_uncen.loc[data_uncen[\"Race_ord\"]==2,:].sample(n = 500) #Randomly select 500 uncenored subjects from the black group of uncensored data\n",
    "    uncen_asian_500=data_uncen.loc[data_uncen[\"Race_ord\"]==3,:].sample(n = 500) #Randomly select 500 uncenored subjects from the asian group of uncensored data\n",
    "    uncen_hispanic_500=data_uncen.loc[data_uncen[\"Race_ord\"]==4,:].sample(n = 500) #Randomly select 500 uncenored subjects from the hispanic group of uncensored data\n",
    "\n",
    "    \n",
    "    uncenored_data=pd.concat([uncen_white_500, uncen_black_500, uncen_asian_500, uncen_hispanic_500])  #Concatenate randomly selected 500+500+500+500=2000 uncensored subjects from the white, black, asian and hispanic groups of uncensored data\n",
    "\n",
    "    \n",
    "    increment_cenored_white_250=data_cen.loc[data_cen[\"Race_ord\"]==1,:].sample(n = 250) #Select 250 cenored subjects from the white group of censored data\n",
    "    increment_cenored_black_250=data_cen.loc[data_cen[\"Race_ord\"]==2,:].sample(n = 250) #Select 250 cenored subjects from the black group of censored data\n",
    "    increment_cenored_asian_250=data_cen.loc[data_cen[\"Race_ord\"]==3,:].sample(n = 250) #Select 250 cenored subjects from the asian group of censored data\n",
    "    increment_cenored_hispanic_250=data_cen.loc[data_cen[\"Race_ord\"]==4,:].sample(n = 250) #Select 250 cenored subjects from the hispanic group of censored data\n",
    "\n",
    "    \n",
    "    increment_all=pd.concat([uncenored_data, increment_cenored_white_250, increment_cenored_black_250, increment_cenored_asian_250, increment_cenored_hispanic_250]).sample(frac=1.0) ## Create Increment All data by respectively adding 500 censored cenored subjects from the white, black, asian and hispanic group and female group of censored data to uncensored dataset\n",
    "    increment_majority=pd.concat([uncenored_data, increment_cenored_white_250]).sample(frac=1.0) ## Create Increment Majority data by only adding 250 censored cenored subjects from the white group of censored data to uncensored dataset\n",
    "    increment_minority=pd.concat([uncenored_data, increment_cenored_black_250, increment_cenored_asian_250, increment_cenored_hispanic_250]).sample(frac=1.0) ## Create Increment Minority data by only adding 250 censored cenored subjects from the black, asian and hispanic group of censored data to uncensored dataset\n",
    "\n",
    "\n",
    "    induced_cenored_white_250=uncen_white_500.sample(n = 250) #Randomly select 500 cenored subjects from the white group of uncensored data\n",
    "    induced_cenored_white_250['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_white_250=uncen_white_500[~uncen_white_500.index.isin(induced_cenored_white_250.index)] #Extract the white uncensored data where censoring is not induced\n",
    "\n",
    "    induced_cenored_black_250=uncen_black_500.sample(n = 250) #Randomly select 500 cenored subjects from the black group of uncensored data\n",
    "    induced_cenored_black_250['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_black_250=uncen_black_500[~uncen_black_500.index.isin(induced_cenored_black_250.index)] #Extract the black uncensored data where censoring is not induced\n",
    "\n",
    "    induced_cenored_asian_250=uncen_asian_500.sample(n = 250) #Randomly select 500 cenored subjects from the asian group of uncensored data\n",
    "    induced_cenored_asian_250['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_asian_250=uncen_asian_500[~uncen_asian_500.index.isin(induced_cenored_asian_250.index)] #Extract the asian uncensored data where censoring is not induced\n",
    "\n",
    "    induced_cenored_hispanic_250=uncen_hispanic_500.sample(n = 250) #Randomly select 500 cenored subjects from the hispanic group of uncensored data\n",
    "    induced_cenored_hispanic_250['status']=0 #Change the event status uncensored to censored (1 to 0)\n",
    "    induced_uncenored_hispanic_250=uncen_hispanic_500[~uncen_hispanic_500.index.isin(induced_cenored_hispanic_250.index)] #Extract the hispanic uncensored data where censoring is not induced\n",
    "    \n",
    "\n",
    "    induced_majority=pd.concat([induced_uncenored_white_250, induced_cenored_white_250, uncen_black_500, uncen_asian_500, uncen_hispanic_500]).sample(frac=1.0) ## Create Induced Majority data by inducing 500 censored subjects to the white group of uncensored data \n",
    "    induced_minority=pd.concat([induced_uncenored_black_250, induced_cenored_black_250, induced_uncenored_asian_250, induced_cenored_asian_250, induced_uncenored_hispanic_250, induced_cenored_hispanic_250, uncen_white_500]).sample(frac=1.0) ## Create Induced Minority data by inducing 500 censored subjects to the black, asian and hispanic group of uncensored data \n",
    "    \n",
    "    return uncenored_data, increment_majority, increment_minority, induced_majority, induced_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4065e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of uncensored: (6396, 15)\n",
      "Dimension of censored: (18923, 15)\n",
      "Percentage of white censored: 54.44%\n",
      "Percentage of black censored: 9.97%\n",
      "Percentage of asian censored: 25.83%\n",
      "Percentage of hispanic censored: 9.76%\n",
      "No. of uncensored white: 3506\n",
      "No. of uncensored black: 1002\n",
      "No. of uncensored asian: 1319\n",
      "No. of uncensored hispanic: 569\n",
      "No. of censored white: 10301\n",
      "No. of censored black: 1887\n",
      "No. of censored asian: 4888\n",
      "No. of censored hispanic: 1847\n"
     ]
    }
   ],
   "source": [
    "uncenored_data, increment_majority, increment_minority, induced_majority, induced_minority=different_cenored_seer('/home/local/AD/mrahman6/0.KDD_2022/0.FISA_FINAL_EXPERIMENT/Data/SEER/seer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd683e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
